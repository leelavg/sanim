#!/bin/bash
set -euo pipefail

# Parse command line arguments
MAP_ONLY=false
while [[ $# -gt 0 ]]; do
  case $1 in
    -m|--map-only)
      MAP_ONLY=true
      shift
      ;;
    *)
      echo "Unknown option: $1" >&2
      echo "Usage: $0 [-m|--map-only]" >&2
      echo "  -m, --map-only  Only generate/update node-zone-map ConfigMap (requires oc)" >&2
      exit 1
      ;;
  esac
done

# Source user configuration if exists
[ -f config.env ] && source config.env

# Apply defaults (user values take precedence)
NAMESPACE="${NAMESPACE:-sanim-system}"
INSTALL_GLOBAL="${INSTALL_GLOBAL:-true}"
INSTALL_ZONAL="${INSTALL_ZONAL:-true}"
GLOBAL_DISK_COUNT="${GLOBAL_DISK_COUNT:-2}"
GLOBAL_DISK_SIZE="${GLOBAL_DISK_SIZE:-10Gi}"
ZONAL_DISK_COUNT="${ZONAL_DISK_COUNT:-1}"
ZONAL_DISK_SIZE="${ZONAL_DISK_SIZE:-10Gi}"
STORAGE_CLASS="${STORAGE_CLASS:-gp3-csi}"
IMAGE="${IMAGE:-ghcr.io/leelavg/sanim:latest}"
NODE_LABEL_FILTER="${NODE_LABEL_FILTER:-node-role.kubernetes.io/worker=}"
IQN_PREFIX="iqn.2020-05.com.thoughtexpo:storage"

SCRIPT_DIR="$(dirname "${BASH_SOURCE[0]}")/scripts"
STS_GLOBAL_SCRIPT=$(cat "$SCRIPT_DIR/sts-global.sh")
STS_ZONAL_SCRIPT=$(cat "$SCRIPT_DIR/sts-zonal.sh")
DS_INIT_SCRIPT=$(cat "$SCRIPT_DIR/ds-init.sh")
READINESS_GLOBAL_SCRIPT=$(cat "$SCRIPT_DIR/readiness-global.sh")
READINESS_ZONAL_SCRIPT=$(cat "$SCRIPT_DIR/readiness-zonal.sh")

# ============================================================================
# NETWORK-DEPENDENT FUNCTION (requires oc CLI)
# ============================================================================
# Queries cluster to build node-to-zone mapping data using jsonpath
# This function makes network calls using 'oc' command
# Side effect: also writes zones.txt with unique zones for offline generation
generate_zone_map_configmap() {
  local mapping=""
  mapping=$(oc get nodes -o jsonpath='{range .items[*]}{"    "}{.status.addresses[?(@.type=="InternalIP")].address}{"="}{.metadata.labels.topology\.kubernetes\.io/zone}{"\n"}{end}' -l "$NODE_LABEL_FILTER")

  if [ -z "$mapping" ]; then
    echo "Error: No nodes with zone labels found" >&2
    exit 1
  fi

  # Extract unique zones from mapping and write to zones.txt
  echo "$mapping" | cut -d= -f2 | sort -u > zones.txt

  cat <<EOF
#, Namespace for sanim resources
---
apiVersion: v1
kind: Namespace
metadata:
  name: ${NAMESPACE}
  labels:
    app.kubernetes.io/name: sanim

#, ConfigMap for node-to-zone mapping
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-zone-map
  namespace: ${NAMESPACE}
  labels:
    app.kubernetes.io/name: sanim
data:
  mapping: |
${mapping}
EOF
}
# ============================================================================

# Handle -m flag: only generate/update zone-map ConfigMap
if [ "$MAP_ONLY" == "true" ]; then
  echo "Using 'oc' to query node to zone mapping"
  generate_zone_map_configmap > node-zone-map.yaml
  echo "Generated node-zone-map.yaml and zones.txt successfully!"
  echo "Apply with: oc apply -f node-zone-map.yaml --server-side --force-conflicts"
  exit 0
fi

# Validation (hard stop before any YAML generation)
if [ "$INSTALL_GLOBAL" != "true" ] && [ "$INSTALL_ZONAL" != "true" ]; then
  echo "Error: At least one of INSTALL_GLOBAL or INSTALL_ZONAL must be set to 'true'" >&2
  exit 1
fi

# Require zones.txt (generated by -m flag) for zone selection
if [ ! -f "zones.txt" ]; then
  echo "Error: zones.txt not found. Run 'bash generate.sh -m' first to query cluster zones." >&2
  exit 1
fi

# Read zones from file (offline, no network calls)
ZONES=($(cat zones.txt))
if [ ${#ZONES[@]} -eq 0 ]; then
  echo "Error: No zones found in zones.txt" >&2
  exit 1
fi

# Use first zone for global target
GLOBAL_ZONE="${ZONES[0]}"
echo "Using zones from zones.txt: ${ZONES[@]}" >&2
echo "Global target will use zone: $GLOBAL_ZONE" >&2

# Start generating resources.yaml
cat > resources.yaml <<YAML
#, Namespace for sanim resources
---
apiVersion: v1
kind: Namespace
metadata:
  name: ${NAMESPACE}
  labels:
    app.kubernetes.io/name: sanim

#, ConfigMap containing entrypoint scripts
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: scripts
  namespace: ${NAMESPACE}
  labels:
    app.kubernetes.io/name: sanim
data:
  sts-global.sh: |
$(echo "$STS_GLOBAL_SCRIPT" | sed 's/^/    /')
  sts-zonal.sh: |
$(echo "$STS_ZONAL_SCRIPT" | sed 's/^/    /')
  ds-init.sh: |
$(echo "$DS_INIT_SCRIPT" | sed 's/^/    /')
  readiness-global.sh: |
$(echo "$READINESS_GLOBAL_SCRIPT" | sed 's/^/    /')
  readiness-zonal.sh: |
$(echo "$READINESS_ZONAL_SCRIPT" | sed 's/^/    /')

#, SecurityContextConstraints for iSCSI targets
---
apiVersion: security.openshift.io/v1
kind: SecurityContextConstraints
metadata:
  name: sanim-target
  labels:
    app.kubernetes.io/name: sanim
allowHostDirVolumePlugin: true
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
# Using privileged: true is more honest than CAP_SYS_ADMIN (which is almost equivalent)
allowPrivilegedContainer: true
allowedCapabilities: null
defaultAddCapabilities: null
fsGroup:
  type: RunAsAny
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities: null
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users:
- system:serviceaccount:${NAMESPACE}:sanim-target
volumes:
- configMap
- emptyDir
- hostPath
- persistentVolumeClaim

#, SecurityContextConstraints for iSCSI initiators
---
apiVersion: security.openshift.io/v1
kind: SecurityContextConstraints
metadata:
  name: sanim-initiator
  labels:
    app.kubernetes.io/name: sanim
allowHostDirVolumePlugin: true
allowHostIPC: false
allowHostNetwork: true
allowHostPID: true
allowHostPorts: false
# Using privileged: true is more honest than CAP_SYS_ADMIN (which is almost equivalent)
allowPrivilegedContainer: true
allowedCapabilities: null
defaultAddCapabilities: null
fsGroup:
  type: RunAsAny
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities: null
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users:
- system:serviceaccount:${NAMESPACE}:sanim-initiator
volumes:
- configMap
- hostPath

#, ServiceAccount for iSCSI targets
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sanim-target
  namespace: ${NAMESPACE}

#, ServiceAccount for iSCSI initiators
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sanim-initiator
  namespace: ${NAMESPACE}

YAML

# Generate Global STS resources if enabled
if [ "$INSTALL_GLOBAL" == "true" ]; then
  cat >> resources.yaml <<YAML
#, StatefulSet for global shared iSCSI target
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: global
  namespace: ${NAMESPACE}
  labels:
    app.kubernetes.io/name: sanim
    app.kubernetes.io/component: target
    sanim.io/type: global
spec:
  serviceName: global-service
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: sanim
      app.kubernetes.io/component: target
      sanim.io/type: global
  template:
    metadata:
      labels:
        app.kubernetes.io/name: sanim
        app.kubernetes.io/component: target
        sanim.io/type: global
    spec:
      serviceAccountName: sanim-target
      automountServiceAccountToken: false
      terminationGracePeriodSeconds: 30
      # Using pod networking for proper Service routing
      # Note: iSCSI sessions will break on pod restart - initiators must reconnect
      nodeSelector:
        topology.kubernetes.io/zone: ${GLOBAL_ZONE}
      containers:
      - name: target
        image: ${IMAGE}
        command: ["/bin/bash", "/scripts/sts-global.sh"]
        env:
        - name: IQN_PREFIX
          value: "${IQN_PREFIX}"
        - name: GLOBAL_DISK_COUNT
          value: "${GLOBAL_DISK_COUNT}"
        securityContext:
          privileged: true
        volumeMounts:
        - name: scripts
          mountPath: /scripts
        - name: lib-modules
          mountPath: /lib/modules
          readOnly: true
        - name: target-config
          mountPath: /etc/target
        - name: dbus
          mountPath: /var/run/dbus
          readOnly: true
        - name: sys-kernel-config
          mountPath: /sys/kernel/config
        volumeDevices:
YAML

  # Generate volumeDevices for strict LUN mapping
  for i in $(seq 0 $((GLOBAL_DISK_COUNT - 1))); do
    cat >> resources.yaml <<YAML
        - name: global-$i
          devicePath: /dev/global-$i
YAML
  done

  cat >> resources.yaml <<YAML
        readinessProbe:
          exec:
            command:
            - /scripts/readiness-global.sh
          initialDelaySeconds: 10
          periodSeconds: 5
      volumes:
      - name: scripts
        configMap:
          name: scripts
          defaultMode: 0755
      - name: lib-modules
        hostPath:
          path: /lib/modules
      - name: target-config
        emptyDir: {}
      - name: dbus
        hostPath:
          path: /var/run/dbus
      - name: sys-kernel-config
        hostPath:
          path: /sys/kernel/config
  volumeClaimTemplates:
YAML

  # Generate PVC templates for global LUNs
for i in $(seq 0 $((GLOBAL_DISK_COUNT - 1))); do
    cat >> resources.yaml <<YAML
  - metadata:
      name: global-$i
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: ${STORAGE_CLASS}
      volumeMode: Block
      resources:
        requests:
          storage: ${GLOBAL_DISK_SIZE}
YAML
  done

  cat >> resources.yaml <<YAML

#, Headless Service for global target (provides stable pod DNS)
---
apiVersion: v1
kind: Service
metadata:
  name: global-service
  namespace: ${NAMESPACE}
  labels:
    app.kubernetes.io/name: sanim
    app.kubernetes.io/component: target
    sanim.io/type: global
spec:
  clusterIP: None
  selector:
    app.kubernetes.io/name: sanim
    app.kubernetes.io/component: target
    sanim.io/type: global
  ports:
  - name: iscsi
    port: 3260
    targetPort: 3260
    protocol: TCP

YAML
fi

# Generate Zonal STS resources if enabled (1 StatefulSet + Service per zone)
if [ "$INSTALL_ZONAL" == "true" ]; then
  echo "Generating zonal resources for ${#ZONES[@]} zones: ${ZONES[@]}" >&2

  for ZONE in "${ZONES[@]}"; do
    # Sanitize zone name for DNS-safe resource names (replace dots with dashes)
    ZONE_SAFE=$(echo "$ZONE" | tr '.' '-')

    cat >> resources.yaml <<YAML
#, StatefulSet for zone ${ZONE}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zonal-${ZONE_SAFE}
  namespace: ${NAMESPACE}
  labels:
    app.kubernetes.io/name: sanim
    app.kubernetes.io/component: target
    sanim.io/type: zonal
    sanim.io/zone: ${ZONE}
spec:
  serviceName: zonal-${ZONE_SAFE}-service
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: sanim
      app.kubernetes.io/component: target
      sanim.io/type: zonal
      sanim.io/zone: ${ZONE}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: sanim
        app.kubernetes.io/component: target
        sanim.io/type: zonal
        sanim.io/zone: ${ZONE}
    spec:
      serviceAccountName: sanim-target
      automountServiceAccountToken: false
      terminationGracePeriodSeconds: 30
      nodeSelector:
        topology.kubernetes.io/zone: ${ZONE}
      containers:
      - name: target
        image: ${IMAGE}
        command: ["/bin/bash", "/scripts/sts-zonal.sh"]
        env:
        - name: IQN_PREFIX
          value: "${IQN_PREFIX}"
        - name: ZONAL_DISK_COUNT
          value: "${ZONAL_DISK_COUNT}"
        - name: NODE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        securityContext:
          privileged: true
        volumeMounts:
        - name: scripts
          mountPath: /scripts
        - name: lib-modules
          mountPath: /lib/modules
          readOnly: true
        - name: target-config
          mountPath: /etc/target
        - name: dbus
          mountPath: /var/run/dbus
          readOnly: true
        - name: sys-kernel-config
          mountPath: /sys/kernel/config
        - name: zone-map
          mountPath: /etc/zone-map
          readOnly: true
        volumeDevices:
YAML

    # Generate volumeDevices for this zone's LUNs
    for i in $(seq 0 $((ZONAL_DISK_COUNT - 1))); do
      cat >> resources.yaml <<YAML
        - name: zonal-$i
          devicePath: /dev/zonal-$i
YAML
    done

    cat >> resources.yaml <<YAML
        readinessProbe:
          exec:
            command:
            - /scripts/readiness-zonal.sh
          initialDelaySeconds: 10
          periodSeconds: 5
      volumes:
      - name: scripts
        configMap:
          name: scripts
          defaultMode: 0755
      - name: lib-modules
        hostPath:
          path: /lib/modules
      - name: target-config
        emptyDir: {}
      - name: dbus
        hostPath:
          path: /var/run/dbus
      - name: sys-kernel-config
        hostPath:
          path: /sys/kernel/config
      - name: zone-map
        configMap:
          name: node-zone-map
  volumeClaimTemplates:
YAML

    # Generate PVC templates for this zone's LUNs
    for i in $(seq 0 $((ZONAL_DISK_COUNT - 1))); do
      cat >> resources.yaml <<YAML
  - metadata:
      name: zonal-$i
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: ${STORAGE_CLASS}
      volumeMode: Block
      resources:
        requests:
          storage: ${ZONAL_DISK_SIZE}
YAML
    done

    # Generate headless Service for this zone
    cat >> resources.yaml <<YAML

#, Headless Service for zone ${ZONE}
---
apiVersion: v1
kind: Service
metadata:
  name: zonal-${ZONE_SAFE}-service
  namespace: ${NAMESPACE}
  labels:
    app.kubernetes.io/name: sanim
    app.kubernetes.io/component: target
    sanim.io/type: zonal
    sanim.io/zone: ${ZONE}
spec:
  clusterIP: None
  selector:
    app.kubernetes.io/name: sanim
    app.kubernetes.io/component: target
    sanim.io/type: zonal
    sanim.io/zone: ${ZONE}
  ports:
  - name: iscsi
    port: 3261
    targetPort: 3261
    protocol: TCP

YAML
  done
fi

# Generate DaemonSet for initiators
cat >> resources.yaml <<YAML
#, DaemonSet for iSCSI initiators (dumb controller)
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: initiator
  namespace: ${NAMESPACE}
  labels:
    app.kubernetes.io/name: sanim
    app.kubernetes.io/component: initiator
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: sanim
      app.kubernetes.io/component: initiator
  template:
    metadata:
      labels:
        app.kubernetes.io/name: sanim
        app.kubernetes.io/component: initiator
    spec:
      serviceAccountName: sanim-initiator
      automountServiceAccountToken: false
      terminationGracePeriodSeconds: 30
      hostNetwork: true
      hostPID: true
      dnsPolicy: ClusterFirstWithHostNet
YAML

# Add nodeSelector if NODE_LABEL_FILTER is set
if [ -n "$NODE_LABEL_FILTER" ]; then
  LABEL_KEY="${NODE_LABEL_FILTER%%=*}"
  LABEL_VALUE="${NODE_LABEL_FILTER#*=}"
  cat >> resources.yaml <<YAML
      nodeSelector:
        ${LABEL_KEY}: "${LABEL_VALUE}"
YAML
fi

cat >> resources.yaml <<YAML
      containers:
      - name: initiator
        image: ${IMAGE}
        command: ["/bin/bash", "/scripts/ds-init.sh"]
        env:
        - name: NAMESPACE
          value: "${NAMESPACE}"
        - name: IQN_PREFIX
          value: "${IQN_PREFIX}"
        - name: INSTALL_GLOBAL
          value: "${INSTALL_GLOBAL}"
        - name: INSTALL_ZONAL
          value: "${INSTALL_ZONAL}"
        - name: NODE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        securityContext:
          privileged: true
        terminationMessagePath: /tmp/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - name: scripts
          mountPath: /scripts
        - name: dev
          mountPath: /dev
          mountPropagation: HostToContainer
        - name: iscsi-config
          mountPath: /etc/iscsi
          mountPropagation: HostToContainer
        - name: iscsi-lib
          mountPath: /var/lib/iscsi
          mountPropagation: HostToContainer
        - name: zone-map
          mountPath: /etc/zone-map
          readOnly: true
      volumes:
      - name: scripts
        configMap:
          name: scripts
          defaultMode: 0755
      - name: dev
        hostPath:
          path: /dev
      - name: iscsi-config
        hostPath:
          path: /etc/iscsi
      - name: iscsi-lib
        hostPath:
          path: /var/lib/iscsi
      - name: zone-map
        configMap:
          name: node-zone-map
YAML

echo "Generated resources.yaml successfully!"
echo "Deploy with: oc apply -f resources.yaml --server-side --force-conflicts"
