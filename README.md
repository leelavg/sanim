# sanim - SAN Simulator

## Foreword (by me - human)

I wanted an in-cluster solution to simulate SAN without a dedicated storage array. Brushed up the basics by reading this [excellent](https://oneuptime.com/blog/post/2026-01-07-ubuntu-iscsi-storage/view) blog and successfully simulated iscsi target, initiator with podman finalizing mounts, networking, and a rough flow, then bounced off ideas about moving this to OCP with Gemini before implementing the flow. All the code is implemented using IBM Bob(shell).
- [prep.txt](./support/prep.txt) given to Gemini based on all the conversations.
- [prompt.txt](./support/prompt.txt) and [hints.txt](./support/hints.txt) is generated by Gemini and provided to IBM Bob.
- [summary.txt](./support/summary.txt) is the changelog of sorts being updated by IBM Bob during implementation sessions.

The core idea remained the same throughout, support a shared-storage (global target) and shared-nothing (zonal target) configurations where the former allows initiator from all zones to login and the latter restricts login to the target in its zone. Statefulset for target is chosen to forgo manual management of PVCs and get DNS resolvable headless service, a simple Daemonset for initiator and a ConfigMap with entrypoint scripts for all the pods.

Everything else follows implementing above idea and bringing it to a workable state, please note even though care is taken to be coherent this is AI generated (partially reviewed so far!) covering multiple cases. The same procedure should work for exporting your LVM disks as well as iSCSI!

`./generate.sh -m` does a survey of zones to node mapping, uses `oc` (a network call). `./generate.sh` uses the zonal info but doesn't do any network calls. Nevertheless, the idea is for you to look at all the generated resources before applying them, the generated files are `zones.txt`, `resources.yaml` and `node-zone-map.yaml`. See [Cleanup](#cleanup) for teardown.

```
> grep '^#,' *yaml -n
node-zone-map.yaml:1:#, Namespace for sanim resources
node-zone-map.yaml:10:#, ConfigMap for node-to-zone mapping
resources.yaml:1:#, Namespace for sanim resources
resources.yaml:10:#, ConfigMap containing entrypoint scripts
resources.yaml:353:#, ServiceAccount for sanim pods
resources.yaml:361:#, SecurityContextConstraints for iSCSI targets
resources.yaml:399:#, SecurityContextConstraints for iSCSI initiators
resources.yaml:434:#, ServiceAccount for iSCSI targets
resources.yaml:442:#, ServiceAccount for iSCSI initiators
resources.yaml:450:#, StatefulSet for global shared iSCSI target
resources.yaml:556:#, Headless Service for global target (provides stable pod DNS)
resources.yaml:579:#, StatefulSet for zone us-east-1d
resources.yaml:685:#, Headless Service for zone us-east-1d
resources.yaml:710:#, DaemonSet for iSCSI initiators (dumb controller)
```

I had looked at docs of [piraeus-operator](https://github.com/piraeusdatastore/piraeus-operator), [jiva-operator](https://github.com/openebs-archive/jiva-operator) and [longhorn](https://github.com/longhorn/longhorn), either I had to shave off something from those or build from start to get to shaved off existing solutions and I had to choose the latter.

## AI Generated

**sanim** (SAN Simulator) is a zero-dependency Bash generator that provides iSCSI block storage on AWS/OpenShift clusters using Fedora 43 containers. Battle-tested through live cluster deployment with comprehensive fixes applied.

## Overview

sanim enables you to quickly provision iSCSI block storage within your Kubernetes/OpenShift cluster for testing, development, or ephemeral workloads. It uses StatefulSets as iSCSI targets and a DaemonSet as a "dumb" initiator controller.

## Architecture

**Components:**
- **STS-Global** (optional): Single-replica StatefulSet on port 3260, auto-uses first zone from zones.txt
- **STS-Zonal** (optional): Per-zone StatefulSets on port 3261, one per zone in zones.txt
- **DS-Initiator**: DaemonSet with sysfs-based health checks (10s interval) and auto-recovery
- **Headless Services**: Stable pod DNS (global-0.global-service, zonal-{zone}-0.zonal-{zone}-service)
- **ConfigMaps**: Entrypoint scripts + node-zone-map
- **SCCs**: Separate SecurityContextConstraints for targets (hostNetwork, hostPorts) and initiators (hostPID)

**Key Design:**
- Pod networking for targets (not hostNetwork) - stable DNS, no port conflicts
- Port separation: global=3260, zonal=3261 (kernel binds at host level, not per-IQN)
- Zone map workflow: `generate.sh -m` queries cluster once, writes zones.txt + node-zone-map.yaml
- Session persistence: Host kernel manages iSCSI sessions, survives pod restarts
- nsenter pattern: Run host's iscsiadm from container (Fedora 43 incompatible with RHCOS)
- No clearconfig/restoreconfig: Prevented kernel listener thread from restarting
- Aggressive timeouts: noop_out_timeout=5s, replacement_timeout=15s
- Hardcoded IQN: `iqn.2020-05.com.thoughtexpo:storage` (domain registration date)
- Hardcoded device prefixes: `global-*` and `zonal-*`

**Flow:**
```
config.env → generate.sh -m → zones.txt + node-zone-map.yaml → oc apply node-zone-map.yaml
config.env → generate.sh → resources.yaml → oc apply resources.yaml
                                                ↓
                              ┌─────────────────┴─────────────────┐
                              ↓                                   ↓
                      STS (Target)                        DS (Initiator)
                      - targetcli config                  - nsenter iscsiadm
                      - Port 3260/3261                    - Sysfs monitoring
                      - LUNs via volumeDevices            - Host kernel sessions
```

## FAQ

**Why StatefulSets for targets?**
Provide stable DNS names (`global-0.global-service`), sticky PVCs across restarts, and zone affinity with `WaitForFirstConsumer`.

**Why is the DaemonSet "dumb"?**
It only runs `iscsiadm` login via `nsenter` - does NOT start `iscsid`. Host kernel manages sessions, so they survive pod restarts (no I/O disruption).

**Why use nsenter for iscsiadm?**
Fedora 43's iscsiadm (6.2.1.11) is incompatible with RHCOS kernel. `nsenter -t 1 -m -u -n -i /usr/sbin/iscsiadm` runs the host's version.

**Why different ports for global vs zonal?**
Port binding happens at host kernel level, not per-IQN. If global and zonal targets co-locate on same node, they'd conflict on port 3260. Solution: global=3260, zonal=3261.

**Target mounts:** volumeDevices for PVC block devices, /var/run/dbus (ro) for targetcli, /sys/kernel/config for configfs.

**Initiator mounts:** /dev, /etc/iscsi, /var/lib/iscsi with HostToContainer propagation. Sessions persist in host kernel across container restarts.

**Cleanup:** See cleanup section below for proper sequence to remove all resources, sessions, and SELinux configuration.

## Usage

### 1. Configure

Edit `config.env`:

```bash
NAMESPACE=sanim-system
INSTALL_GLOBAL=true
INSTALL_ZONAL=true
GLOBAL_DISK_COUNT=2
GLOBAL_DISK_SIZE=10Gi
ZONAL_DISK_COUNT=1
ZONAL_DISK_SIZE=10Gi
STORAGE_CLASS=gp3-csi
IMAGE=ghcr.io/leelavg/sanim:latest
NODE_LABEL_FILTER=node-role.kubernetes.io/worker=
```

**Hardcoded:** IQN=`iqn.2020-05.com.thoughtexpo:storage`, device prefixes=`global-*/zonal-*`, ports=`3260/3261`

### 2. Generate Zone Map

Query cluster for node-to-zone mapping (requires oc):

```bash
bash generate.sh -m
oc apply -f node-zone-map.yaml --server-side --force-conflicts
```

This creates `zones.txt` and `node-zone-map.yaml`.

### 3. Generate and Deploy

```bash
bash generate.sh  # Reads zones.txt offline, auto-selects first zone for global
oc apply -f resources.yaml --server-side --force-conflicts
```

### 4. Verify

```bash
# Check components
oc get sts,ds,pods -n sanim-system

# Verify iSCSI sessions on host kernel
oc exec -n sanim-system <initiator-pod> -- nsenter -t 1 -m -u -n -i /usr/sbin/iscsiadm --mode session

# Inspect all generated resources
grep '^#,' *.yaml
```

## Configuration Reference

| Variable | Default | Description |
|----------|---------|-------------|
| `NAMESPACE` | `sanim-system` | Namespace for all resources |
| `INSTALL_GLOBAL` | `false` | Enable global shared target (uses first zone from zones.txt) |
| `INSTALL_ZONAL` | `false` | Enable zonal shared-nothing targets (one per zone in zones.txt) |
| `GLOBAL_DISK_COUNT` | `2` | Number of LUNs for global target |
| `GLOBAL_DISK_SIZE` | `10Gi` | Size of each global LUN |
| `ZONAL_DISK_COUNT` | `1` | Number of LUNs per zonal target |
| `ZONAL_DISK_SIZE` | `10Gi` | Size of each zonal LUN |
| `STORAGE_CLASS` | `gp3-csi` | StorageClass (must support volumeMode: Block) |
| `IMAGE` | `ghcr.io/leelavg/sanim:latest` | Container image |
| `NODE_LABEL_FILTER` | `node-role.kubernetes.io/worker=` | Node selector for initiators |

**Hardcoded:** IQN=`iqn.2020-05.com.thoughtexpo:storage`, devices=`global-*/zonal-*`, ports=3260/3261

**Zone management:** Run `generate.sh -m` first to create zones.txt (offline, no network calls during generation)

## Building the Container Image

```bash
podman build -t ghcr.io/leelavg/sanim:latest -f Containerfile .
podman push ghcr.io/leelavg/sanim:latest
```

The container includes:
- `targetcli-fb` for iSCSI target configuration
- `util-linux` for nsenter
- Debug tools: `bind-utils`, `iputils`, `tcpdump`

## iSCSI Concepts

**IQN Format:** `iqn.YYYY-MM.reverse.domain:identifier`
- Global: `iqn.2020-05.com.thoughtexpo:storage:global`
- Zonal: `iqn.2020-05.com.thoughtexpo:storage:us-west-2b` (example)
- Initiator: `iqn.1994-05.com.redhat:{node}` (host)

**Target vs Initiator:**
- Target: STS pods export storage via targetcli
- Initiator: DS pods consume storage via host's iscsiadm

**Key Commands (via nsenter):**
```bash
# Discover targets
nsenter -t 1 -m -u -n -i /usr/sbin/iscsiadm --mode discovery --type sendtargets --portal <DNS>

# Login
nsenter -t 1 -m -u -n -i /usr/sbin/iscsiadm --mode node --targetname <IQN> --portal <DNS> --login

# View sessions
nsenter -t 1 -m -u -n -i /usr/sbin/iscsiadm --mode session
```

## Timeouts and Intervals

The system uses several timeouts for session management and pod lifecycle:

**iSCSI Session Timeouts (ds-init.sh):**
- noop_out_timeout: 5s - Detects unresponsive connections
- replacement_timeout: 15s - Time before declaring session dead
  (Monitor loop checks every 10s and reconnects via DNS, so actual recovery is ~10-15s)

**Pod Termination:**
- terminationGracePeriodSeconds: 30s - Grace period for cleanup (targets and initiators)
  Allows trap handler to delete IQN, backstores, and logout sessions cleanly

**Monitoring:**
- Session health check interval: 10s - Monitor loop checks sysfs and reconnects if unhealthy
- Readiness probe: initialDelaySeconds=10s, periodSeconds=5s

**Retry Logic (ds-init.sh):**
- Discovery retries: 5 attempts with 2s delay
- Login retries: 5 attempts with 2s delay

## Troubleshooting

**No LUNs in target:**
```bash
oc get pvc -n sanim-system  # Check PVCs bound
oc exec -n sanim-system <target-pod> -- ls -la /dev/global-* /dev/zonal-*
```

**Discovery failures:**
```bash
oc exec -n sanim-system <initiator-pod> -- getent hosts global-service.sanim-system.svc.cluster.local
oc exec -n sanim-system <target-pod> -- targetcli /iscsi ls
```

**Kernel logs (critical for login failures):**
```bash
# From node
oc debug node/<node> → chroot /host → dmesg | grep -i iscsi

# From pod (if /dev/kmsg accessible)
oc exec -n sanim-system <initiator-pod> -- cat /dev/kmsg | grep -i iscsi
```

Common errors: `detected conn error` (network), `session recovery timed out` (target unreachable), `Login failed` (config issue)

**Zone mismatch:** `oc get pods -n sanim-system -o custom-columns=NAME:.metadata.name,ZONE:.metadata.labels.'topology\.kubernetes\.io/zone'`

**DNS issues:** Verify `dnsPolicy: ClusterFirstWithHostNet` for hostNetwork pods

## Security Considerations

- sanim requires privileged containers and host access
- Use only in trusted environments (dev/test clusters)
- Not recommended for production workloads
- No CHAP authentication configured by default

## Multipath Configuration (not tested)

On OpenShift/RHCOS nodes with multipathd, blacklist sanim devices in `/etc/multipath.conf`:

```
blacklist {
    device {
        vendor "LIO-ORG"
        product ".*"
    }
}
```

Or by IQN: `wwid "iqn.2020-05.com.thoughtexpo:storage.*"`

Reload: `systemctl reload multipathd`

## Cleanup

**WARNING**: The cleanup commands below will logout ALL iSCSI sessions and delete ALL iSCSI node records on the nodes. If you have other iSCSI setups on these nodes, DO NOT run these commands. Instead, manually logout only sanim sessions using specific IQNs.

Proper cleanup sequence to avoid orphaned sessions and SELinux configuration:

```bash
# 1. Delete DaemonSet first (stops session monitoring)
oc delete daemonset initiator -n sanim-system

# 2. Logout from ALL iSCSI sessions and delete ALL node records (DESTRUCTIVE!)
for node in $(oc get nodes -l "$(grep NODE_LABEL_FILTER config.env | cut -d= -f2)" -o name | cut -d/ -f2); do
  echo "Cleaning iSCSI on $node..."
  oc debug node/$node -q -ndefault -- chroot /host bash -c "/usr/sbin/iscsiadm --mode node --logoutall=all 2>/dev/null || true; rm -rf /var/lib/iscsi/nodes/* /var/lib/iscsi/send_targets/*" 2>/dev/null || true
done

# 3. Remove SELinux port 3261 configuration from nodes with matching label filter
for node in $(oc get nodes -l "$(grep NODE_LABEL_FILTER config.env | cut -d= -f2)" -o name | cut -d/ -f2); do
  echo "Removing SELinux config on $node..."
  oc debug node/$node -q -ndefault -- chroot /host /usr/sbin/semanage port -d -t iscsi_port_t -p tcp 3261 2>/dev/null || true
done

# 4. Delete all remaining resources
oc delete -f resources.yaml --ignore-not-found=true
```

**Note**: PVCs are retained by default. To delete them:

```bash
oc delete pvc -n sanim-system --all
```

## Requirements

- OpenShift 4.x or Kubernetes 1.20+
- **StorageClass with `volumeMode: Block` and `WaitForFirstConsumer` binding**
- Nodes with `topology.kubernetes.io/zone` labels
- Cluster admin (for SCC creation)

Verify: `oc get sc <your-class> -o yaml | grep -E 'volumeBindingMode|volumeMode'`

## Examples

See `examples/` directory for usage examples:

- **examples/shared-storage/**: Demonstrates global target with multi-node access
  - `writer.yaml` and `reader.yaml` - data written from one node is readable from another
- **examples/shared-nothing/**: Demonstrates zonal targets with zone isolation
  - `zonal-test.yaml` - DaemonSet showing all nodes in zone can access zonal storage

Each example includes a README with usage instructions.

## Validation

Run by human

``` sh
> ./scripts/validate.sh
==========================================
sanim Validation Script
==========================================

Checking zones.txt... ✓ (1 zones: us-east-1d)
Checking namespace sanim-system... ✓
Checking ConfigMap scripts... ✓
Checking ConfigMap node-zone-map... ✓
Checking SCC sanim-target... ✓
Checking SCC sanim-initiator... ✓

Global Target Validation:
-------------------------
  StatefulSet... ✓
  Service... ✓
  Pod ready... ✓
  PVCs bound... ✓ (2/2)
  iSCSI target configured... ✓
  Port 3260 listening... ✓
  Target details:
    o- iscsi .............................................................................................................. [Targets: 2]
      o- iqn.2020-05.com.thoughtexpo:storage:global .......................................................................... [TPGs: 1]
      | o- tpg1 .................................................................................................... [gen-acls, no-auth]
      |   o- acls ............................................................................................................ [ACLs: 0]
      |   o- luns ............................................................................................................ [LUNs: 2]
      |   | o- lun0 .................................................................... [block/lun0 (/dev/global-0) (default_tg_pt_gp)]
      |   | o- lun1 .................................................................... [block/lun1 (/dev/global-1) (default_tg_pt_gp)]
      |   o- portals ...................................................................................................... [Portals: 1]
      |     o- [::0]:3260 ......................................................................................................... [OK]
      o- iqn.2020-05.com.thoughtexpo:storage:us-east-1d ...................................................................... [TPGs: 1]
        o- tpg1 .................................................................................................... [gen-acls, no-auth]
          o- acls ............................................................................................................ [ACLs: 0]
          o- luns ............................................................................................................ [LUNs: 1]
          | o- lun0 ............................................................... [block/zonal-lun0 (/dev/zonal-0) (default_tg_pt_gp)]
          o- portals ...................................................................................................... [Portals: 1]
            o- 0.0.0.0:3261 ....................................................................................................... [OK]

Zonal Targets Validation:
-------------------------
  Found 1 zonal StatefulSet(s)

  Zone: us-east-1d (zonal-us-east-1d)
    Service... ✓
    Pod ready... ✓
    PVCs bound... ✓ (1/1)
    iSCSI target... ✓
    Port 3261 listening... ✓

Initiator Validation:
---------------------
  DaemonSet... ✓
  Pods ready... ✓ (2/2)
  Kernel iSCSI sessions (via nsenter, sample 3 nodes):
    initiator-k6brn (ip-10-0-1-216.ec2.internal, ):
      tcp: [6] 10.129.0.24:3260,1 iqn.2020-05.com.thoughtexpo:storage:global (non-flash)
      tcp: [7] 10.129.0.30:3261,1 iqn.2020-05.com.thoughtexpo:storage:us-east-1d (non-flash)
      Session health (sysfs):
        session6: LOGGED_IN
        session7: LOGGED_IN
    initiator-ndrb5 (ip-10-0-1-192.ec2.internal, ):
      tcp: [10] 10.129.0.24:3260,1 iqn.2020-05.com.thoughtexpo:storage:global (non-flash)
      tcp: [11] 10.129.0.30:3261,1 iqn.2020-05.com.thoughtexpo:storage:us-east-1d (non-flash)
      Session health (sysfs):
        session10: LOGGED_IN
        session11: LOGGED_IN
  ✓ Found active sessions on 2 node(s)

==========================================
Validation Summary
==========================================
Global target: ✓ Ready
Zonal targets: ✓ 1/1 zones ready
Initiators: ✓ 2/2 nodes

Next steps:
  - View sessions on node: oc exec -n sanim-system <initiator-pod> -- nsenter -t 1 -m -u -n -i /usr/sbin/iscsiadm -m session
  - Check kernel logs: oc debug node/<node> → chroot /host → dmesg | grep -i iscsi
  - List block devices: lsblk | grep -E 'sd[b-z]'
  - Test I/O: dd if=/dev/zero of=/dev/sdX bs=1M count=100 oflag=direct
```

## License

Licensed under the Apache License, Version 2.0. See [LICENSE](LICENSE) file for details.

## Contributing

Contributions welcome! Please ensure:
- Scripts remain zero-dependency (pure Bash)
- Follow existing patterns for heredocs and indentation
- Test with both global and zonal configurations
- Update documentation for new features
- Scripts in `scripts/` directory maintain full IDE syntax highlighting

