SANIM - SAN SIMULATOR (PRODUCTION-READY)
==========================================

Project Overview:
Zero-dependency Bash generator for iSCSI block storage on Kubernetes/OpenShift using
Fedora 43 containers. Battle-tested through live cluster deployment with comprehensive
fixes from two major testing sessions.

Architecture:
- STS-Global: Single-replica StatefulSet (port 3260) exporting cluster-wide LUNs
- STS-Zonal: Per-zone StatefulSet (port 3261) with shared-nothing architecture
- DS-Initiator: DaemonSet with sysfs-based session monitoring (10s interval, 15s timeout)
- Headless Services: Stable pod DNS (global-0.global-service, zonal-{zone}-0.zonal-{zone}-service)
- ConfigMap: Entrypoint scripts (sts-global.sh, sts-zonal.sh, ds-init.sh) + node-zone-map
- SCC: Custom SecurityContextConstraints with separate SAs for targets and initiators

Key Architecture Decisions:
1. Pod networking for targets (no hostNetwork) - stable DNS, avoids port conflicts
2. Per-zone StatefulSets - one STS+Service per zone, not 1 STS with N replicas
3. Port separation - global uses 3260, zonal uses 3261 (kernel binds at host level)
4. Zone map optimization - 'generate.sh -m' creates zones.txt + node-zone-map.yaml offline
5. Session persistence - host kernel manages sessions, survives pod restarts
6. No clearconfig/restoreconfig - prevented kernel listener thread from restarting
7. Generic functions - check_session_health() and discover_and_login() in ds-init.sh
8. Sysfs monitoring - structured session health checks via /sys/class/iscsi_session/*/state
9. Aggressive timeouts - noop_out_timeout=5s, replacement_timeout=15s for fast failure detection

Critical Fixes (Live Testing):
- Portal listener bug: Removed clearconfig/restoreconfig cycle that stopped kernel listener
- Port binding conflict: Global uses 3260, zonal uses 3261 (binding is per-host-kernel, not per-IQN)
- SELinux port blocking: Port 3261 requires explicit addition to iscsi_port_t policy (Red Hat Solution 5170611)
- SELinux auto-config: DaemonSet configures all nodes via nsenter/semanage on startup
- TPG portal changes: Must disable TPG before modifying portals, then re-enable
- Session auto-recovery: Monitor loop checks sysfs every 10s, reconnects on failure
- Zonal login simplified: Connects only to own zone (no multi-zone sweep)
- Startup cleanup: Only login if no healthy session exists (prevents stale session accumulation)
- DNS resolution: dnsPolicy: ClusterFirstWithHostNet for hostNetwork pods
- iscsiadm compatibility: nsenter to host's iscsiadm (Fedora 43 incompatible with RHCOS kernel)

Configuration (config.env):
- INSTALL_GLOBAL/INSTALL_ZONAL: Enable/disable deployment types
- GLOBAL_DISK_COUNT/SIZE: Global LUN configuration (default: 2x10Gi)
- ZONAL_DISK_COUNT/SIZE: Per-zone LUN configuration (default: 1x10Gi)
- STORAGE_CLASS: Must support volumeMode: Block (default: gp3-csi)
- NODE_LABEL_FILTER: Node selector for initiators (default: node-role.kubernetes.io/worker=)
- IMAGE: Container image path

Hardcoded (not configurable):
- IQN: iqn.2020-05.com.thoughtexpo:storage (domain registration date)
- Device prefixes: global-* and zonal-*
- Ports: 3260 (global), 3261 (zonal)

Zone Management:
- Run 'generate.sh -m' first - queries cluster nodes, writes zones.txt + node-zone-map.yaml
- Then 'generate.sh' - reads zones.txt offline, auto-selects first zone for global target

Deployment Flow:
1. Survey cluster: oc get nodes --show-labels
2. Generate zone map: bash generate.sh -m (creates zones.txt + node-zone-map.yaml)
3. Apply zone map: oc apply -f node-zone-map.yaml --server-side --force-conflicts
4. Edit config.env with disk sizes and node filter
5. Generate resources: bash generate.sh (reads zones.txt offline)
6. Deploy: oc apply -f resources.yaml --server-side --force-conflicts
7. Validate: bash scripts/validate.sh (checks kernel sessions)

Cleanup Flow:
1. Delete DaemonSet: oc delete daemonset initiator -n sanim-system
2. Logout sessions on labeled nodes via oc debug
3. Remove SELinux port 3261 on labeled nodes via oc debug
4. Delete resources: oc delete -f resources.yaml --ignore-not-found=true
See README.md for complete commands with label filter extraction from config.env

Key Files:
- generate.sh: Main generator with -m flag for zone mapping
- scripts/sts-global.sh: Global target entrypoint (port 3260)
- scripts/sts-zonal.sh: Zonal target entrypoint (port 3261, TPG disable/enable sequence)
- scripts/ds-init.sh: Initiator with SELinux auto-config and session health monitoring
- scripts/validate.sh: Validation script for sessions and block devices
- config.env: User configuration
- Containerfile: Fedora 43 with targetcli and debug tools
- resources.yaml: Generated K8s resources (Namespace, ConfigMaps, SCCs, SAs, STSs, Services, DS)
- README.md: Includes deployment flow and proper cleanup sequence

IQN and Device Naming:
- Global IQN: iqn.2020-05.com.thoughtexpo:storage:global
- Zonal IQN: iqn.2020-05.com.thoughtexpo:storage:{zone} (e.g., us-west-2b)
- Initiator IQN: iqn.1994-05.com.redhat:{node-id} (from RHCOS host)
- Global devices: /dev/global-0, /dev/global-1
- Zonal devices: /dev/zonal-0, /dev/zonal-1

Target Flow (sts-global.sh, sts-zonal.sh):
- Load kernel modules: target_core_mod, iscsi_target_mod
- Mount /sys/kernel/config (configfs), /var/run/dbus (targetcli communication)
- Clean orphaned objects in configfs (if present)
- Configure iSCSI targets via targetcli
- Export PVC block devices as LUNs (via volumeDevices)
- Global binds ::0:3260, zonal binds 0.0.0.0:3261
- Run 'targetcli saveconfig' (no clearconfig/restoreconfig)
- No CHAP authentication

Initiator Flow (ds-init.sh):
- Configure SELinux: Add port 3261 to iscsi_port_t via nsenter/semanage (runs on ALL nodes)
- Use nsenter to run host's iscsiadm (Fedora 43 compatibility)
- Check for existing healthy sessions via sysfs (skip login if found)
- Discover targets via headless Service DNS
- Login with aggressive timeouts (noop_out_timeout=5s, replacement_timeout=15s)
- Monitor loop: check sysfs every 10s with interruptible wait, reconnect on failure
- Zonal: Only connect to own zone target (uses node-zone-map)
- Sessions persist in host kernel across pod restarts
- Mounts: /dev, /etc/iscsi, /var/lib/iscsi (HostToContainer propagation)

Security:
- Custom SCCs: sanim-target (hostNetwork, hostPorts) and sanim-initiator (hostPID)
- Separate ServiceAccounts bound to respective SCCs
- automountServiceAccountToken: false (no K8s API access needed)
- Requires cluster admin for SCC creation

Multipath Configuration (not tested):
Add to /etc/multipath.conf on worker nodes:
```
blacklist {
    device {
        vendor "LIO-ORG"
        product ".*"
    }
}
```
Or blacklist by IQN: `wwid "iqn.2020-05.com.thoughtexpo:storage.*"`
Reload: `systemctl reload multipathd`

Troubleshooting:
- Kernel logs: `dmesg | grep -i iscsi` or `oc debug node/<name> → chroot /host → journalctl -k | grep -i iscsi`
- Session health: Check /sys/class/iscsi_session/session*/state (should show LOGGED_IN)
- Validation: `bash validate.sh` (checks kernel sessions and block devices)
- Common errors: Connection timeouts (check Service DNS), login failures (check kernel logs), port conflicts (verify global=3260, zonal=3261)

Known Limitations:
- No CHAP authentication (dev/test only)
- Requires privileged containers and host access
- StorageClass must support volumeMode: Block
- Not recommended for production workloads
- Multipath must be manually configured

Testing Session (Feb 16, 2026):
- Validated all components on live cluster, all tests passed
- Finalized target lifecycle: transient config with trap-based cleanup
- Established aggressive timeouts for fast failure detection
- Validated readiness probes cover full target availability

Repository: leelavg/sanim
Domain: thoughtexpo.com (May 2020)
Status: Production-ready, battle-tested on live OpenShift clusters

Testing Session (Feb 17, 2026) - Examples:
- Created examples/shared-storage/ with writer.yaml and reader.yaml
  - Demonstrates multi-node access to global target via magic marker synchronization
- Created examples/shared-nothing/ with zonal-test.yaml (DaemonSet)
  - Demonstrates zone-isolated storage with all nodes in zone accessing zonal target
- Both examples validated successfully on 2-node cluster
